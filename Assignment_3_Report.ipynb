{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3_Report",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prinakk/A3_ML_UTS2019/blob/master/Assignment_3_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTxSkf9b8CkY",
        "colab_type": "text"
      },
      "source": [
        "# Real-Time Sign Language Translator\n",
        "Prina Kamakotti\n",
        "\n",
        "13171780"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVxhQl4X8zLB",
        "colab_type": "text"
      },
      "source": [
        "# AIM\n",
        "The main objective of this project is to create an real-time sign language interpreter in the hopes of promoting communication between people with hearing and speech disability and the world of hearing. To facilitate automated translation, we use Machine learning models. Machine Learning is an application of artificial intelligence which is used to analyse data and apply the results in other situations automatically. Machine Learning enables computers to learn and grow from the data provided, which mimics a human beings ability to grow from past experiences. The main goals of this project are as follow:\n",
        "\n",
        "1.   Image processing to recognise hand gestures.\n",
        "2.   Select appropriate machine learning model and train the model with various sign languages.\n",
        "3.   Validate and deploy the project.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdbw9zog9DZN",
        "colab_type": "text"
      },
      "source": [
        "# BACKGROUND\n",
        "\n",
        "Translating is a tough endeavour even amongst spoken languages, let alone translation of sign language. Sign language is the main form of communication of people with hearing or speech disabilities. According to the World Health Organization, any type of hearing or speech impairment affects about 5 per cent of the world's population. This is a vast number of people affected, nearly 450 million human beings. So sign language to them is the choice of communication. Like every spoken language, which has variations and dialects according to the location of origin, the world currently has more than a hundred different sign languages. To people with no hearing or speech disabilities, learning sign language is usually a neglected cause, hence communicating with people who have such disabilities is extremely arduous. \n",
        "\n",
        "\n",
        "Machine translation of sign language has been available from 1977, albeit with limited capabilities. Sign Language translation began through the use of hand gesture recognition equipment such as gloves with motion sensors etc. This method to interpret hand gestures has a severe limitation with its efficiency and the movement restrictions. So, with the current digital age development, cameras and digital recordings replaced the old hand glove hardware, making it much easier to interpret sign language.  As seen in the objectives, the first hurdle in the translation process is to recognise the hand gestures, and a literature review of this problem shows that there have been several methods to address this issue. Cohen et al. (2003) proposed an ensemble method which used Hidden Markov Models, which is a statistical machine learning method along with traditional machine learning models. Boulay et al. (2003) developed a method to recognise human postures in video sequences with the help of a three-dimensional human model and to compare the sequence with it. From the literature review, It can be assumed that the many proposed methods can all be used to identify the hand gesture, and it is advised to choose the most suitable method for the data.\n",
        "\n",
        "The main point of differentiation between the traditional and the proposed method is the fact that the proposed method can perform translations in real-time, contrary to the traditional methods where the translation should be a pre-recorded digital video or photographs. Also, the points of similarity with the traditional methods lie in the recognition stage, where similar methods are used to extract the hand gestures from the person signing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PMVnjgcTfiE",
        "colab_type": "text"
      },
      "source": [
        "# Automated Sign Translator\n",
        "\n",
        "## Introduction\n",
        "Any real-time application of machine learning requires two things, a front-end that captures the data, in this case, it is the video or pictures from a user's webcam and a back-end which should return the predicted translation to be displayed. The front end of the project is responsible for collecting the hand gestures, and a function should be provided to split the video in frames to capture all the hand signs. These gestures are then fed to the back-end, which consists of the recognition model and the prediction model, both of which make use of machine learning algorithms. As the video captured might not only consist of the hand but also the whole human body, it is essential to include a bounding box to isolate the hand signs from the environment. The boundary box can be interfaced on the screen, and the user will have to perform the sign language within the bounding box present in the display. After this is developed the first phase of the project would be complete. Since the back-end is responsible for the recognition and prediction, there should be a significant emphasis on the specifications in this phase of the project, few of the key specifications are as follow: \n",
        "\n",
        "1. **Data Gathering**:  As mentioned in the above sections, there are many sign languages present in the world, hence gathering data about them is crucial. Since this is an unconventional method of sign language translation, data sets for most of the sign languages are not present, hence it is advised to create a dataset for the words required. And this can be achieved by downloading videos of people performing sign language and enclosing a bounding box at each sign with the meaning behind them. This part of the project will have the most manual human work. \n",
        "\n",
        "2. **Selecting a Machine learning Model for Real-Time recognition:** The second phase of the back-end development consists of selecting and training a machine learning model. Since this project deals with identifying an object, algorithms which deal with object identifying like YOLO(You Only Look Once) algorithms or algorithms like Long Short Term Memory which is suitable for time-series predictions.\n",
        "\n",
        "3. **Designing the User Interface:** In this stage of the project, it is crucial to make the application user-friendly, as it should be easy to use for people with little or no technical background. Hence the application can be widely used and can bring about benefits to all. \n",
        "\n",
        "\n",
        "\n",
        "## Significance and Innovation\n",
        "\n",
        "As mentioned in the above sections, the importance of such an application cannot be stressed enough, an application which can produce a real-time translation of sign language benefits the above mentioned 450 million people with speaking or hearing disabilities in the world. Imagine what a person with such disability as that has to go through, a small task like ordering coffee might be immeasurable hard since the other person might not be aware of sign language. Lots of opportunities are lost to such people due to the negligence of the importance of sign language. One such common example would be in the recruitment field; at this time and age, most companies use video conferencing tools to do interviews for jobs, for people who have said disabilities, this is a lost opportunity. Another aspect is the fact that this gives the people with said disabilities a way to fit in; they can be understood; they can get their words across without an interpreter.\n",
        "\n",
        "The innovation in this project is that it provides real-time sign language translation, conventional sign language communication approaches use digital recordings of signing and perform recognition on it. As machine learning is a robust technology, it is possible to train many sign languages and all of them can be translated.\n",
        "\n",
        "\n",
        "## Project Timeline\n",
        "\n",
        "\n",
        " ![Gnatt Chart](https://drive.google.com/uc?id=1POYgUICVDPSP-k5WufCnUJXvfeibCwME)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG5xuAnfDVt7",
        "colab_type": "text"
      },
      "source": [
        "# Budget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBa3G2QR9Lw5",
        "colab_type": "text"
      },
      "source": [
        "# Video Pitch Link\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvpYfQiO7-gN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# GitHub Link\n",
        "\n",
        "https://github.com/prinakk/A3_ML_UTS2019"
      ]
    }
  ]
}